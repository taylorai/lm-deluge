## LM Deluge Major Updates (0.0.90)

* **Proxy server:** spin up a local webserver which serves OpenAI and Anthropic endpoints, backed by any model supported by lm-deluge. You can use this for e.g. proxying Claude Code to Gemini. This is:
  * (a) Not actually recommended for Claude Code, since models and harnesses are co-trained and Gemini won't work well in Claude Code. But it's fun that this is even possible!
  * (b) Not recommended for serious production use, since it's new, unstable, and has rough edges. But it makes interesting integrations possible, e.g. **verifiers**. If you want a serious proxy server, use LiteLLM.
* **Tinker Support:** Tinker supports sampling from any pre-trained checkpoint with their OpenAI-compatible API. We added support for any Tinker model, simply by passing the tinker:// URI and providing your TINKER_API_KEY.
* **Seatbelt Sandbox:** All sandbox prefab tools have been updated to have a consistent API, that better matches the Claude Code Bash tool. There's also a new type of sandbox that runs commands *locally* on a Mac using `sandbox-exec`. I would not recommend relying on this for untrusted code, but hey we all do it every day by using Claude Code/Codex, so you can decide based on your own risk tolerance.



