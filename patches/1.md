diff --git a/src/lm_deluge/api_requests/base.py b/src/lm_deluge/api_requests/base.py
index 2c7c01f4aad36fdbd5e6309fa52f837d23f355bb..03f6e2511e975a95533f192a44923d96c0b90fda 100644
--- a/src/lm_deluge/api_requests/base.py
+++ b/src/lm_deluge/api_requests/base.py
@@ -1,216 +1,113 @@
 import asyncio
-import random
 import traceback
 from abc import ABC, abstractmethod

 import aiohttp
 from aiohttp import ClientResponse

-from lm_deluge.models import APIModel
-
-from ..config import SamplingParams
 from ..errors import raise_if_modal_exception
 from ..request_context import RequestContext
 from .response import APIResponse


 class APIRequestBase(ABC):
     """
     Class for handling API requests. All model/endpoint-specific logic should be
     handled by overriding __init__ and implementing the handle_response method.
     For call_api to work, the __init__ must handle setting:
         - url
         - request_header
         - request_json
     """

     def __init__(
         self,
         context: RequestContext,
     ):
         # If context is provided, use it; otherwise construct one from individual parameters
         self.context = context

         # Everything is now accessed through self.context - no copying!
         self.system_prompt = None
         self.result = []  # list of APIResponse objects from each attempt

         # these should be set in the __init__ of the subclass
         self.url = None
         self.request_header = None
         self.request_json = None
         self.region = None

     def increment_pbar(self):
         if self.context.status_tracker:
             self.context.status_tracker.increment_pbar()

-    def call_callback(self):
-        if self.context.callback is not None:
-            # the APIResponse in self.result includes all the information
-            self.context.callback(self.result[-1], self.context.status_tracker)
-
-    def handle_success(self, data):
-        self.call_callback()
-        if self.context.status_tracker:
-            self.context.status_tracker.task_succeeded(self.context.task_id)
-
-    def handle_error(self, create_new_request=False, give_up_if_no_other_models=False):
-        """
-        If create_new_request is True, will create a new API request (so that it
-        has a chance of being sent to a different model). If false, will retry
-        the same request.
-        """
-        assert self.context.status_tracker
-        last_result: APIResponse = self.result[-1]
-        error_to_print = f"Error  task {self.context.task_id}. "
-        error_to_print += (
-            f"Model: {last_result.model_internal} Code: {last_result.status_code}, "
-        )
-        if self.region is not None:
-            error_to_print += f"Region: {self.region}, "
-        error_to_print += f"Message: {last_result.error_message}."
-        print(error_to_print)
-        if self.context.attempts_left > 0:
-            self.context.attempts_left -= 1
-            if not create_new_request:
-                assert self.context.status_tracker.retry_queue
-                self.context.status_tracker.retry_queue.put_nowait(self)
-                return
-            else:
-                # make sure we have another model to send it to besides the current one
-                if (
-                    self.context.all_model_names is None
-                    or len(self.context.all_model_names) < 2
-                ):
-                    if give_up_if_no_other_models:
-                        print(
-                            f"No other models to try for task {self.context.task_id}. Giving up."
-                        )
-                        self.context.status_tracker.task_failed(self.context.task_id)
-                    else:
-                        print(
-                            f"No other models to try for task {self.context.task_id}. Retrying with same model."
-                        )
-                        assert self.context.status_tracker.retry_queue
-                        self.context.status_tracker.retry_queue.put_nowait(self)
-                else:
-                    # two things to change: model_name and sampling_params
-                    new_model_name = self.context.model_name
-                    new_model_idx = 0
-                    while new_model_name == self.context.model_name:
-                        new_model_idx = random.randint(
-                            0, len(self.context.all_model_names) - 1
-                        )
-                        new_model_name = self.context.all_model_names[new_model_idx]
-
-                    if isinstance(self.context.all_sampling_params, list):
-                        new_sampling_params = self.context.all_sampling_params[
-                            new_model_idx
-                        ]
-                    elif isinstance(self.context.all_sampling_params, SamplingParams):
-                        new_sampling_params = self.context.all_sampling_params
-                    elif self.context.all_sampling_params is None:
-                        new_sampling_params = self.context.sampling_params
-                    else:
-                        new_sampling_params = self.context.sampling_params
-
-                    print("Creating new request with model", new_model_name)
-                    # Create new context with updated model and sampling params
-                    new_context = self.context.copy(
-                        model_name=new_model_name, sampling_params=new_sampling_params
-                    )
-                    new_model_obj = APIModel.from_registry(new_model_name)
-                    new_request = new_model_obj.make_request(new_context)
-                    # PROBLEM: new request is never put into results array, so we can't get the result.
-                    assert self.context.status_tracker.retry_queue
-                    self.context.status_tracker.retry_queue.put_nowait(self)
-                    # SOLUTION: just need to make sure it's deduplicated by task_id later.
-                    assert self.context.results_arr
-                    self.context.results_arr.append(new_request)
-        else:
-            print(f"Task {self.context.task_id} out of tries.")
-            self.context.status_tracker.task_failed(self.context.task_id)
-
-    async def call_api(self):
+    async def call_api(self) -> APIResponse:
+        """Execute the HTTP request and return the parsed APIResponse."""
         assert self.context.status_tracker
         try:
             self.context.status_tracker.total_requests += 1
             timeout = aiohttp.ClientTimeout(total=self.context.request_timeout)
             async with aiohttp.ClientSession(timeout=timeout) as session:
                 assert self.url is not None, "URL is not set"
                 async with session.post(
                     url=self.url,
                     headers=self.request_header,
                     json=self.request_json,
                 ) as http_response:
-                    response: APIResponse = await self.handle_response(http_response)
-
+                    response = await self.handle_response(http_response)
             self.result.append(response)
-            if response.is_error:
-                self.handle_error(
-                    create_new_request=response.retry_with_different_model or False,
-                    give_up_if_no_other_models=response.give_up_if_no_other_models
-                    or False,
-                )
-            else:
-                self.handle_success(response)
-
+            return response
         except asyncio.TimeoutError:
-            self.result.append(
-                APIResponse(
-                    id=self.context.task_id,
-                    model_internal=self.context.model_name,
-                    prompt=self.context.prompt,
-                    sampling_params=self.context.sampling_params,
-                    status_code=None,
-                    is_error=True,
-                    error_message="Request timed out (terminated by client).",
-                    content=None,
-                    usage=None,
-                )
+            response = APIResponse(
+                id=self.context.task_id,
+                model_internal=self.context.model_name,
+                prompt=self.context.prompt,
+                sampling_params=self.context.sampling_params,
+                status_code=None,
+                is_error=True,
+                error_message="Request timed out (terminated by client).",
+                content=None,
+                usage=None,
             )
-            self.handle_error(create_new_request=False)
-
+            self.result.append(response)
+            return response
         except Exception as e:
             raise_if_modal_exception(e)
             tb = traceback.format_exc()
             print(tb)
-            self.result.append(
-                APIResponse(
-                    id=self.context.task_id,
-                    model_internal=self.context.model_name,
-                    prompt=self.context.prompt,
-                    sampling_params=self.context.sampling_params,
-                    status_code=None,
-                    is_error=True,
-                    error_message=f"Unexpected {type(e).__name__}: {str(e) or 'No message.'}",
-                    content=None,
-                    usage=None,
-                )
+            response = APIResponse(
+                id=self.context.task_id,
+                model_internal=self.context.model_name,
+                prompt=self.context.prompt,
+                sampling_params=self.context.sampling_params,
+                status_code=None,
+                is_error=True,
+                error_message=f"Unexpected {type(e).__name__}: {str(e) or 'No message.'}",
+                content=None,
+                usage=None,
             )
-            # maybe consider making True?
-            self.handle_error(create_new_request=False)
+            self.result.append(response)
+            return response

     @abstractmethod
     async def handle_response(self, http_response: ClientResponse) -> APIResponse:
         raise NotImplementedError


 def deduplicate_responses(results: list[APIRequestBase]) -> list[APIResponse]:
     deduplicated = {}
     for request in results:
         if request.context.task_id not in deduplicated:
             deduplicated[request.context.task_id] = request.result[-1]
         else:
             current_response: APIResponse = deduplicated[request.context.task_id]
             # only replace if the current request has no completion and the new one does
             if (
                 request.result[-1].completion is not None
                 and current_response.completion is None
             ):
                 deduplicated[request.context.task_id] = request.result[-1]

     output = [deduplicated[request.context.task_id] for request in results]

     return output
diff --git a/src/lm_deluge/client.py b/src/lm_deluge/client.py
index e561914dac27f77102e25f289b5f516ab0363690..f5dbd079d22543bbea9716d6794ed310db1207c3 100644
--- a/src/lm_deluge/client.py
+++ b/src/lm_deluge/client.py
@@ -1,43 +1,43 @@
 import asyncio
 from typing import Any, Literal, Self, Sequence, overload

 import numpy as np
 import yaml
 from pydantic import BaseModel
 from pydantic.functional_validators import model_validator

 from lm_deluge.api_requests.openai import stream_chat
 from lm_deluge.batches import (
     submit_batches_anthropic,
     submit_batches_oa,
     wait_for_batch_completion_async,
 )
 from lm_deluge.prompt import CachePattern, Conversation, prompts_to_conversations
 from lm_deluge.tool import Tool

-from .api_requests.base import APIRequestBase, APIResponse, deduplicate_responses
+from .api_requests.base import APIResponse
 from .config import SamplingParams
 from .models import APIModel, registry
 from .request_context import RequestContext
 from .tracker import StatusTracker

 # from .cache import LevelDBCache, SqliteCache


 # TODO: get completions as they finish, not all at once at the end.
 # relatedly, would be nice to cache them as they finish too.
 # TODO: add optional max_input_tokens to client so we can reject long prompts to prevent abuse
 class LLMClient(BaseModel):
     """
     LLMClient abstracts all the fixed arguments to process_prompts_async, so you can create it
     once and use it for more stuff without having to configure all the arguments.
     Handles models, sampling params for each model, model weights, rate limits, etc.
     """

     model_names: list[str] = ["gpt-4.1-mini"]

     def __init__(self, model_name: str | list[str] | None = None, **kwargs):
         if model_name is not None:
             kwargs["model_names"] = model_name
         super().__init__(**kwargs)

@@ -152,230 +152,221 @@ class LLMClient(BaseModel):
         else:
             config_dict["sampling_params"] = SamplingParams(
                 **config_dict["sampling_params"]
             )

         return cls(**config_dict)

     @classmethod
     def from_yaml(cls, file_path: str):
         config_dict = yaml.safe_load(open(file_path))
         return cls.from_dict(config_dict)

     @classmethod
     def basic(cls, model: str | list[str], **kwargs):
         """
         Doesn't do anything differently now, kept for backwards compat.
         """
         kwargs["model_names"] = model
         return cls(**kwargs)

     def _select_model(self):
         assert isinstance(self.model_weights, list)
         model_idx = np.random.choice(range(len(self.models)), p=self.model_weights)
         return self.models[model_idx], self.sampling_params[model_idx]

+    def _select_different_model(self, current_model: str):
+        """Select a model different from the provided one."""
+        other_models = [m for m in self.models if m != current_model]
+        if not other_models:
+            return current_model, self.sampling_params[self.models.index(current_model)]
+        weights = [self.model_weights[self.models.index(m)] for m in other_models]
+        weights = [w / sum(weights) for w in weights]
+        model_idx = np.random.choice(range(len(other_models)), p=weights)
+        chosen = other_models[model_idx]
+        sp = self.sampling_params[self.models.index(chosen)]
+        return chosen, sp
+
+    async def process_single_request(
+        self, context: RequestContext, sleep_seconds: float
+    ) -> APIResponse:
+        """Handle one request lifecycle including retries and caching."""
+        tracker = context.status_tracker
+        assert tracker is not None
+
+        # Check cache first
+        if self.cache:
+            cached = self.cache.get(context.prompt)
+            if cached:
+                cached.cache_hit = True
+                tracker.task_succeeded(context.task_id)
+                return cached
+
+        attempts_left = context.attempts_left
+        retry_count = 0
+        while attempts_left > 0:
+            # Wait for available capacity
+            while True:
+                tracker.update_capacity()
+                if tracker.check_capacity(context.num_tokens, retry=retry_count > 0):
+                    tracker.set_limiting_factor(None)
+                    break
+                tracker.update_pbar()
+                await asyncio.sleep(sleep_seconds)
+                if tracker.seconds_to_pause > 0:
+                    await asyncio.sleep(tracker.seconds_to_pause)
+                    print(f"Pausing {tracker.seconds_to_pause}s to cool down.")
+
+            model_obj = APIModel.from_registry(context.model_name)
+            request = model_obj.make_request(context)
+            response = await request.call_api()
+
+            if not response.is_error:
+                tracker.task_succeeded(context.task_id)
+                if self.cache and response.completion:
+                    self.cache.put(context.prompt, response)
+                if context.callback:
+                    context.callback(response, tracker)
+                return response
+
+            # Error case
+            attempts_left -= 1
+            retry_count += 1
+            if attempts_left <= 0:
+                tracker.task_failed(context.task_id)
+                if context.callback:
+                    context.callback(response, tracker)
+                return response
+
+            if response.retry_with_different_model:
+                if len(context.all_model_names or []) < 2:
+                    if response.give_up_if_no_other_models:
+                        tracker.task_failed(context.task_id)
+                        if context.callback:
+                            context.callback(response, tracker)
+                        return response
+                new_model, new_sp = self._select_different_model(context.model_name)
+                context = context.copy(
+                    model_name=new_model,
+                    sampling_params=new_sp,
+                    attempts_left=attempts_left,
+                )
+            else:
+                context = context.copy(attempts_left=attempts_left)
+
+        # Should not reach here, but return last response if exists
+        return response
+
     @overload
     async def process_prompts_async(
         self,
         prompts: Sequence[str | list[dict] | Conversation],
         *,
         return_completions_only: Literal[True],
         show_progress: bool = ...,
         tools: list[Tool] | None = ...,
         cache: CachePattern | None = ...,
         computer_use: bool = ...,
         display_width: int = ...,
         display_height: int = ...,
         use_responses_api: bool = ...,
     ) -> list[str | None]: ...

     @overload
     async def process_prompts_async(
         self,
         prompts: Sequence[str | list[dict] | Conversation],
         *,
         return_completions_only: Literal[False] = ...,
         show_progress: bool = ...,
         tools: list[Tool] | None = ...,
         cache: CachePattern | None = ...,
         computer_use: bool = ...,
         display_width: int = ...,
         display_height: int = ...,
         use_responses_api: bool = ...,
     ) -> list[APIResponse | None]: ...

     async def process_prompts_async(
         self,
         prompts: Sequence[str | list[dict] | Conversation],
         *,
         return_completions_only: bool = False,
         show_progress: bool = True,
         tools: list[Tool] | None = None,
         cache: CachePattern | None = None,
         computer_use: bool = False,
         display_width: int = 1024,
         display_height: int = 768,
         use_responses_api: bool = False,
     ) -> list[APIResponse | None] | list[str | None] | dict[str, int]:
-        # if prompts are not Conversations, convert them.
+        # Convert prompts to Conversation objects
         prompts = prompts_to_conversations(prompts)
-        ids = np.arange(len(prompts))
-
-        # if using cache, check for cached completions
-        if self.cache:
-            cached_results = [self.cache.get(prompt) for prompt in prompts]
-            cache_hit_ids = [
-                id for id, res in zip(ids, cached_results) if res is not None
-            ]
-            cache_hit_results = [res for res in cached_results if res is not None]
-            assert len(cache_hit_ids) == len(
-                cache_hit_results
-            ), "Cache hit ids and results must be the same length."
-            remaining_ids = np.array([i for i in ids if i not in cache_hit_ids])
-            remaining_prompts = [prompts[i] for i in remaining_ids]
-            print(
-                f"{len(cache_hit_ids)} cache hits; {len(remaining_ids)} prompts remaining."
-            )
-
-        else:
-            cache_hit_ids = []
-            cache_hit_results = []
-            remaining_prompts = prompts
-            remaining_ids = ids
+        ids = list(range(len(prompts)))

         results: list[APIResponse | None] = [None for _ in range(len(prompts))]
-        if len(remaining_prompts) > 0:
-            # Create StatusTracker with integrated progress bar
-            tracker = StatusTracker(
-                max_requests_per_minute=self.max_requests_per_minute,
-                max_tokens_per_minute=self.max_tokens_per_minute,
-                max_concurrent_requests=self.max_concurrent_requests,
-                use_progress_bar=show_progress,
-                progress_bar_total=len(prompts),
-                progress_bar_disable=not show_progress,
-                use_rich=show_progress,  # Disable Rich if progress is disabled
-            )
-
-            # Initialize progress bar and update with cache hits
-            tracker.init_progress_bar()
-            if len(cache_hit_ids) > 0:
-                tracker.update_pbar(len(cache_hit_ids))
-
-            if isinstance(ids, np.ndarray):
-                ids = ids.tolist()  # pyright: ignore
-
-            # calculate dynamically so we don't throttle RPM
-            seconds_to_sleep_each_loop = (60.0 * 0.9) / tracker.max_requests_per_minute
-            next_request = None  # variable to hold the next request to call
-            prompts_not_finished = True
-            prompts_iter = iter(zip(ids, prompts))
-            requests: list[APIRequestBase] = []
-            assert tracker.retry_queue, "retry queue not initialized"
-            while True:
-                # get next request (if one is not already waiting for capacity)
-                retry_request = False
-                if next_request is None:
-                    if not tracker.retry_queue.empty():
-                        next_request = tracker.retry_queue.get_nowait()
-                        retry_request = True
-                        print(f"Retrying request {next_request.task_id}.")
-                    elif prompts_not_finished:
-                        try:
-                            # get new request
-                            id, prompt = next(prompts_iter)
-                            # select model
-                            model, sampling_params = self._select_model()
-
-                            # Create RequestContext to encapsulate all parameters
-                            context = RequestContext(
-                                task_id=id,
-                                model_name=model,
-                                prompt=prompt,  # type: ignore
-                                sampling_params=sampling_params,
-                                attempts_left=self.max_attempts,
-                                request_timeout=self.request_timeout,
-                                status_tracker=tracker,
-                                results_arr=requests,
-                                all_model_names=self.models,
-                                all_sampling_params=self.sampling_params,
-                                tools=tools,
-                                cache=cache,
-                                computer_use=computer_use,
-                                display_width=display_width,
-                                display_height=display_height,
-                                use_responses_api=use_responses_api,
-                            )
-                            model_obj = APIModel.from_registry(context.model_name)
-                            next_request = model_obj.make_request(context)
-                            requests.append(next_request)
-
-                        except StopIteration:
-                            prompts_not_finished = False
-                            # print("API requests finished, only retries remain.")
-
-                # update available capacity
-                tracker.update_capacity()
-
-                # if enough capacity available, call API
-                if next_request:
-                    next_request_tokens = next_request.context.num_tokens
-                    if tracker.check_capacity(next_request_tokens, retry=retry_request):
-                        tracker.set_limiting_factor(None)
-                        # call API (attempts_left will be decremented in handle_error if it fails)
-                        asyncio.create_task(next_request.call_api())
-                        next_request = None  # reset next_request to empty
-                # update pbar status
-                tracker.update_pbar()

-                # if all tasks are finished, break
-                if tracker.num_tasks_in_progress == 0:
-                    break
+        tracker = StatusTracker(
+            max_requests_per_minute=self.max_requests_per_minute,
+            max_tokens_per_minute=self.max_tokens_per_minute,
+            max_concurrent_requests=self.max_concurrent_requests,
+            use_progress_bar=show_progress,
+            progress_bar_total=len(prompts),
+            progress_bar_disable=not show_progress,
+            use_rich=show_progress,
+        )

-                # main loop sleeps briefly so concurrent tasks can run
-                await asyncio.sleep(seconds_to_sleep_each_loop)
+        tracker.init_progress_bar()
+        seconds_to_sleep_each_loop = (60.0 * 0.9) / tracker.max_requests_per_minute
+
+        async def run_context(task_id: int, prompt: Conversation):
+            model, sampling_params = self._select_model()
+            context = RequestContext(
+                task_id=task_id,
+                model_name=model,
+                prompt=prompt,
+                sampling_params=sampling_params,
+                attempts_left=self.max_attempts,
+                request_timeout=self.request_timeout,
+                status_tracker=tracker,
+                all_model_names=self.models,
+                all_sampling_params=self.sampling_params,
+                tools=tools,
+                cache=cache,
+                computer_use=computer_use,
+                display_width=display_width,
+                display_height=display_height,
+                use_responses_api=use_responses_api,
+            )
+            res = await self.process_single_request(context, seconds_to_sleep_each_loop)
+            results[task_id] = res

-                # if a rate limit error was hit recently, pause to cool down
-                if tracker.seconds_to_pause > 0:
-                    await asyncio.sleep(tracker.seconds_to_pause)
-                    print(f"Pausing {tracker.seconds_to_pause}s to cool down.")
+        tasks = [asyncio.create_task(run_context(i, p)) for i, p in zip(ids, prompts)]
+        if tasks:
+            await asyncio.gather(*tasks)

-            # after finishing, log final status
-            tracker.log_final_status()
-
-            # deduplicate results by id
-            api_results = deduplicate_responses(requests)
-            for res in api_results:
-                results[res.id] = res
-                # set to cache if result has a completion
-                if self.cache and res.completion:
-                    self.cache.put(prompts[res.id], res)
-
-        # add cache hits back in
-        for id, res in zip(cache_hit_ids, cache_hit_results):
-            res.cache_hit = True
-            results[id] = res
+        tracker.log_final_status()

         if return_completions_only:
             return [r.completion if r is not None else None for r in results]

         return results

     def process_prompts_sync(
         self,
         prompts: Sequence[str | list[dict] | Conversation],
         *,
         return_completions_only: bool = False,
         show_progress=True,
         tools: list[Tool] | None = None,
         cache: CachePattern | None = None,
     ):
         return asyncio.run(
             self.process_prompts_async(
                 prompts=prompts,
                 return_completions_only=return_completions_only,
                 show_progress=show_progress,
                 tools=tools,
                 cache=cache,
             )
         )

diff --git a/tests/test_retry_fix.py b/tests/test_retry_fix.py
index b12b15c2ff764ce132a7f8f8837510a34f73ec46..9cd5e8826911e3234da09b3828e95217f7e3aa7e 100644
--- a/tests/test_retry_fix.py
+++ b/tests/test_retry_fix.py
@@ -1,65 +1,63 @@
 #!/usr/bin/env python3
 """Test script to verify the retry bug fix by mocking failures."""

 import asyncio
 from unittest.mock import patch

 from lm_deluge import Conversation, LLMClient, Message
 from lm_deluge.api_requests.base import APIResponse


 async def test_retry_fix():
     """Test that failing requests don't cause infinite retries."""
     print("Testing retry fix with mocked failures...")

     # Create a client with a single model (to trigger the bug path)
     client = LLMClient.basic("gpt-4o-mini")
     client.max_attempts = 3  # Limit attempts for faster test

     # original_call_api = None

     # Mock the API call to always timeout
     async def mock_failing_call_api(self):
         print(
             f"Mock API call for task {self.task_id}, attempts_left: {self.attempts_left}"
         )
-        # Simulate timeout error
-        self.result.append(
-            APIResponse(
-                id=self.task_id,
-                model_internal=self.model_name,
-                prompt=self.prompt,
-                sampling_params=self.sampling_params,
-                status_code=None,
-                is_error=True,
-                error_message="Request timed out (terminated by client).",
-                content=None,
-                usage=None,
-            )
+        response = APIResponse(
+            id=self.task_id,
+            model_internal=self.model_name,
+            prompt=self.prompt,
+            sampling_params=self.sampling_params,
+            status_code=None,
+            is_error=True,
+            error_message="Request timed out (terminated by client).",
+            content=None,
+            usage=None,
         )
-        self.handle_error(create_new_request=False)
+        self.result.append(response)
+        return response

     # Patch the call_api method for all request types
     with patch(
         "lm_deluge.api_requests.openai.OpenAIRequest.call_api", mock_failing_call_api
     ):
         try:
             res = await client.process_prompts_async(
                 [
                     Conversation.system("You are a helpful assistant").add(
                         Message.user().add_text("What's the capital of Paris?")
                     )
                 ],
                 show_progress=False,
             )

             # If we get here, the loop exited properly
             print("✓ Test passed: Loop exited after max attempts")
             print(f"Result: {res[0]}")
             if res[0] and res[0].is_error:
                 print(f"Error message: {res[0].error_message}")

         except Exception as e:
             print(f"✗ Test failed with exception: {e}")
