diff --git a/src/lm_deluge/api_requests/base.py b/src/lm_deluge/api_requests/base.py
index 2c7c01f4aad36fdbd5e6309fa52f837d23f355bb..713cface068f50a07276bf10046d668cf585a6d5 100644
--- a/src/lm_deluge/api_requests/base.py
+++ b/src/lm_deluge/api_requests/base.py
@@ -110,107 +110,106 @@ class APIRequestBase(ABC):
                         ]
                     elif isinstance(self.context.all_sampling_params, SamplingParams):
                         new_sampling_params = self.context.all_sampling_params
                     elif self.context.all_sampling_params is None:
                         new_sampling_params = self.context.sampling_params
                     else:
                         new_sampling_params = self.context.sampling_params

                     print("Creating new request with model", new_model_name)
                     # Create new context with updated model and sampling params
                     new_context = self.context.copy(
                         model_name=new_model_name, sampling_params=new_sampling_params
                     )
                     new_model_obj = APIModel.from_registry(new_model_name)
                     new_request = new_model_obj.make_request(new_context)
                     # PROBLEM: new request is never put into results array, so we can't get the result.
                     assert self.context.status_tracker.retry_queue
                     self.context.status_tracker.retry_queue.put_nowait(self)
                     # SOLUTION: just need to make sure it's deduplicated by task_id later.
                     assert self.context.results_arr
                     self.context.results_arr.append(new_request)
         else:
             print(f"Task {self.context.task_id} out of tries.")
             self.context.status_tracker.task_failed(self.context.task_id)

-    async def call_api(self):
+    async def call_api_once(self) -> APIResponse:
+        """Execute the HTTP request a single time and return the response."""
         assert self.context.status_tracker
         try:
             self.context.status_tracker.total_requests += 1
             timeout = aiohttp.ClientTimeout(total=self.context.request_timeout)
             async with aiohttp.ClientSession(timeout=timeout) as session:
                 assert self.url is not None, "URL is not set"
                 async with session.post(
                     url=self.url,
                     headers=self.request_header,
                     json=self.request_json,
                 ) as http_response:
                     response: APIResponse = await self.handle_response(http_response)

-            self.result.append(response)
-            if response.is_error:
-                self.handle_error(
-                    create_new_request=response.retry_with_different_model or False,
-                    give_up_if_no_other_models=response.give_up_if_no_other_models
-                    or False,
-                )
-            else:
-                self.handle_success(response)
+            return response

         except asyncio.TimeoutError:
-            self.result.append(
-                APIResponse(
-                    id=self.context.task_id,
-                    model_internal=self.context.model_name,
-                    prompt=self.context.prompt,
-                    sampling_params=self.context.sampling_params,
-                    status_code=None,
-                    is_error=True,
-                    error_message="Request timed out (terminated by client).",
-                    content=None,
-                    usage=None,
-                )
+            return APIResponse(
+                id=self.context.task_id,
+                model_internal=self.context.model_name,
+                prompt=self.context.prompt,
+                sampling_params=self.context.sampling_params,
+                status_code=None,
+                is_error=True,
+                error_message="Request timed out (terminated by client).",
+                content=None,
+                usage=None,
             )
-            self.handle_error(create_new_request=False)

-        except Exception as e:
+        except Exception as e:  # pragma: no cover - edge case passthrough
             raise_if_modal_exception(e)
             tb = traceback.format_exc()
             print(tb)
-            self.result.append(
-                APIResponse(
-                    id=self.context.task_id,
-                    model_internal=self.context.model_name,
-                    prompt=self.context.prompt,
-                    sampling_params=self.context.sampling_params,
-                    status_code=None,
-                    is_error=True,
-                    error_message=f"Unexpected {type(e).__name__}: {str(e) or 'No message.'}",
-                    content=None,
-                    usage=None,
-                )
+            return APIResponse(
+                id=self.context.task_id,
+                model_internal=self.context.model_name,
+                prompt=self.context.prompt,
+                sampling_params=self.context.sampling_params,
+                status_code=None,
+                is_error=True,
+                error_message=f"Unexpected {type(e).__name__}: {str(e) or 'No message.'}",
+                content=None,
+                usage=None,
+            )
+
+    async def call_api(self):
+        """Execute the request and apply built-in retry handling."""
+        response = await self.call_api_once()
+
+        self.result.append(response)
+        if response.is_error:
+            self.handle_error(
+                create_new_request=response.retry_with_different_model or False,
+                give_up_if_no_other_models=response.give_up_if_no_other_models or False,
             )
-            # maybe consider making True?
-            self.handle_error(create_new_request=False)
+        else:
+            self.handle_success(response)

     @abstractmethod
     async def handle_response(self, http_response: ClientResponse) -> APIResponse:
         raise NotImplementedError


 def deduplicate_responses(results: list[APIRequestBase]) -> list[APIResponse]:
     deduplicated = {}
     for request in results:
         if request.context.task_id not in deduplicated:
             deduplicated[request.context.task_id] = request.result[-1]
         else:
             current_response: APIResponse = deduplicated[request.context.task_id]
             # only replace if the current request has no completion and the new one does
             if (
                 request.result[-1].completion is not None
                 and current_response.completion is None
             ):
                 deduplicated[request.context.task_id] = request.result[-1]

     output = [deduplicated[request.context.task_id] for request in results]

     return output
diff --git a/src/lm_deluge/api_requests/bedrock.py b/src/lm_deluge/api_requests/bedrock.py
index 84320bca282f0c84fd41ee59397dcac5d26753d5..68c6ce24901a75056d3844bb8d646d72190f6252 100644
--- a/src/lm_deluge/api_requests/bedrock.py
+++ b/src/lm_deluge/api_requests/bedrock.py
@@ -104,136 +104,126 @@ class BedrockRequest(APIRequestBase):
             if self.context.tools:
                 tool_definitions.extend(
                     [tool.dump_for("anthropic") for tool in self.context.tools]
                 )

             # Add cache control to last tool if tools_only caching is specified
             if self.context.cache == "tools_only" and tool_definitions:
                 tool_definitions[-1]["cache_control"] = {"type": "ephemeral"}

             self.request_json["tools"] = tool_definitions

         # Setup AWS4Auth for signing
         self.auth = AWS4Auth(
             self.access_key,
             self.secret_key,
             self.region,
             self.service,
             session_token=self.session_token,
         )

         # Setup basic headers (AWS4Auth will add the Authorization header)
         self.request_header = {
             "Content-Type": "application/json",
         }

-    async def call_api(self):
-        """Override call_api to handle AWS4Auth signing."""
-        try:
-            import aiohttp
-
-            assert self.context.status_tracker
+    async def call_api_once(self) -> APIResponse:
+        """Execute the request once with AWS4Auth signing."""
+        import aiohttp

-            self.context.status_tracker.total_requests += 1
-            timeout = aiohttp.ClientTimeout(total=self.context.request_timeout)
-
-            # Prepare the request data
-            payload = json.dumps(self.request_json, separators=(",", ":")).encode(
-                "utf-8"
-            )
+        assert self.context.status_tracker

-            # Create a fake requests.PreparedRequest object for AWS4Auth to sign
-            import requests
+        self.context.status_tracker.total_requests += 1
+        timeout = aiohttp.ClientTimeout(total=self.context.request_timeout)

-            fake_request = requests.Request(
-                method="POST",
-                url=self.url,
-                data=payload,
-                headers=self.request_header.copy(),
-            )
+        # Prepare the request data
+        payload = json.dumps(self.request_json, separators=(",", ":")).encode("utf-8")

-            # Prepare the request so AWS4Auth can sign it properly
-            prepared_request = fake_request.prepare()
+        # Create a fake requests.PreparedRequest object for AWS4Auth to sign
+        import requests

-            # Let AWS4Auth sign the prepared request
-            signed_request = self.auth(prepared_request)
+        fake_request = requests.Request(
+            method="POST",
+            url=self.url,
+            data=payload,
+            headers=self.request_header.copy(),
+        )

-            # Extract the signed headers
-            signed_headers = dict(signed_request.headers)
+        prepared_request = fake_request.prepare()
+        signed_request = self.auth(prepared_request)
+        signed_headers = dict(signed_request.headers)

+        try:
             async with aiohttp.ClientSession(timeout=timeout) as session:
                 async with session.post(
                     url=self.url,
                     headers=signed_headers,
                     data=payload,
                 ) as http_response:
                     response: APIResponse = await self.handle_response(http_response)
-
-            self.result.append(response)
-            if response.is_error:
-                self.handle_error(
-                    create_new_request=response.retry_with_different_model or False,
-                    give_up_if_no_other_models=response.give_up_if_no_other_models
-                    or False,
-                )
-            else:
-                self.handle_success(response)
+            return response

         except asyncio.TimeoutError:
-            self.result.append(
-                APIResponse(
-                    id=self.context.task_id,
-                    model_internal=self.context.model_name,
-                    prompt=self.context.prompt,
-                    sampling_params=self.context.sampling_params,
-                    status_code=None,
-                    is_error=True,
-                    error_message="Request timed out (terminated by client).",
-                    content=None,
-                    usage=None,
-                )
+            return APIResponse(
+                id=self.context.task_id,
+                model_internal=self.context.model_name,
+                prompt=self.context.prompt,
+                sampling_params=self.context.sampling_params,
+                status_code=None,
+                is_error=True,
+                error_message="Request timed out (terminated by client).",
+                content=None,
+                usage=None,
             )
-            self.handle_error(create_new_request=False)

-        except Exception as e:
+        except Exception as e:  # pragma: no cover - edge case passthrough
             from ..errors import raise_if_modal_exception

             raise_if_modal_exception(e)
-            self.result.append(
-                APIResponse(
-                    id=self.context.task_id,
-                    model_internal=self.context.model_name,
-                    prompt=self.context.prompt,
-                    sampling_params=self.context.sampling_params,
-                    status_code=None,
-                    is_error=True,
-                    error_message=f"Unexpected {type(e).__name__}: {str(e) or 'No message.'}",
-                    content=None,
-                    usage=None,
-                )
+            return APIResponse(
+                id=self.context.task_id,
+                model_internal=self.context.model_name,
+                prompt=self.context.prompt,
+                sampling_params=self.context.sampling_params,
+                status_code=None,
+                is_error=True,
+                error_message=f"Unexpected {type(e).__name__}: {str(e) or 'No message.'}",
+                content=None,
+                usage=None,
             )
-            self.handle_error(create_new_request=False)
+
+    async def call_api(self):
+        response = await self.call_api_once()
+
+        self.result.append(response)
+        if response.is_error:
+            self.handle_error(
+                create_new_request=response.retry_with_different_model or False,
+                give_up_if_no_other_models=response.give_up_if_no_other_models or False,
+            )
+        else:
+            self.handle_success(response)

     async def handle_response(self, http_response: ClientResponse) -> APIResponse:
         is_error = False
         error_message = None
         thinking = None
         content = None
         usage = None
         status_code = http_response.status
         mimetype = http_response.headers.get("Content-Type", None)
         assert self.context.status_tracker

         if status_code >= 200 and status_code < 300:
             try:
                 data = await http_response.json()
                 response_content = data["content"]

                 # Parse response into Message with parts
                 parts = []
                 for item in response_content:
                     if item["type"] == "text":
                         parts.append(Text(item["text"]))
                     elif item["type"] == "thinking":
                         thinking = item["thinking"]
                         parts.append(Thinking(item["thinking"]))
                     elif item["type"] == "tool_use":
diff --git a/src/lm_deluge/client.py b/src/lm_deluge/client.py
index e561914dac27f77102e25f289b5f516ab0363690..4e3506532f1fedb5b6aac7bc500e2b1e88daf706 100644
--- a/src/lm_deluge/client.py
+++ b/src/lm_deluge/client.py
@@ -139,50 +139,93 @@ class LLMClient(BaseModel):
                 registry[model].get("supports_logprobs") for model in self.models
             ):
                 raise ValueError(
                     "logprobs can only be enabled if all models support it."
                 )
         return self

     @classmethod
     def from_dict(cls, config_dict: dict):
         if isinstance(config_dict["sampling_params"], list):
             config_dict["sampling_params"] = [
                 SamplingParams(**x) for x in config_dict["sampling_params"]
             ]
         else:
             config_dict["sampling_params"] = SamplingParams(
                 **config_dict["sampling_params"]
             )

         return cls(**config_dict)

     @classmethod
     def from_yaml(cls, file_path: str):
         config_dict = yaml.safe_load(open(file_path))
         return cls.from_dict(config_dict)

+    async def _execute_request(self, context: RequestContext) -> APIResponse:
+        """Create and execute a single provider request."""
+        model_obj = APIModel.from_registry(context.model_name)
+        request = model_obj.make_request(context)
+        response = await request.call_api_once()
+        return response
+
+    async def process_single_request(self, context: RequestContext) -> APIResponse:
+        """Send one request with retries and caching handled by the client."""
+        if self.cache:
+            cached = self.cache.get(context.prompt)
+            if cached:
+                cached.cache_hit = True
+                if context.status_tracker:
+                    context.status_tracker.task_succeeded(context.task_id)
+                return cached
+
+        last_response: APIResponse | None = None
+        for attempt in range(context.attempts_left):
+            last_response = await self._execute_request(context)
+
+            if not last_response.is_error:
+                if self.cache and last_response.completion:
+                    self.cache.put(context.prompt, last_response)
+                if context.status_tracker:
+                    context.status_tracker.task_succeeded(context.task_id)
+                return last_response
+
+            if attempt < context.attempts_left - 1:
+                if last_response.retry_with_different_model and context.all_model_names:
+                    other_models = [m for m in context.all_model_names if m != context.model_name]
+                    if other_models:
+                        new_model = np.random.choice(other_models)
+                        idx = context.all_model_names.index(new_model)
+                        context.model_name = new_model
+                        if isinstance(context.all_sampling_params, list):
+                            context.sampling_params = context.all_sampling_params[idx]
+
+        if context.status_tracker:
+            context.status_tracker.task_failed(context.task_id)
+        assert last_response is not None
+        return last_response
+
     @classmethod
     def basic(cls, model: str | list[str], **kwargs):
         """
         Doesn't do anything differently now, kept for backwards compat.
         """
         kwargs["model_names"] = model
         return cls(**kwargs)

     def _select_model(self):
         assert isinstance(self.model_weights, list)
         model_idx = np.random.choice(range(len(self.models)), p=self.model_weights)
         return self.models[model_idx], self.sampling_params[model_idx]

     @overload
     async def process_prompts_async(
         self,
         prompts: Sequence[str | list[dict] | Conversation],
         *,
         return_completions_only: Literal[True],
         show_progress: bool = ...,
         tools: list[Tool] | None = ...,
         cache: CachePattern | None = ...,
         computer_use: bool = ...,
         display_width: int = ...,
         display_height: int = ...,
diff --git a/tests/test_client_managed.py b/tests/test_client_managed.py
new file mode 100644
index 0000000000000000000000000000000000000000..c1c3704929f90977df18a5e548b0bc470e30c875
--- /dev/null
+++ b/tests/test_client_managed.py
@@ -0,0 +1,124 @@
+import asyncio
+from unittest.mock import AsyncMock
+import sys
+# ruff: noqa: E402
+
+class DummyEncoding:
+    def encode(self, _):
+        return [0]
+    def decode(self, _):
+        return ""
+
+sys.modules["tiktoken"] = type(
+    "T",
+    (),
+    {"encoding_for_model": lambda *args, **kwargs: DummyEncoding()},
+)()
+
+from lm_deluge import Conversation, LLMClient, SamplingParams
+from lm_deluge.request_context import RequestContext
+from lm_deluge.api_requests.base import APIResponse
+
+
+
+def test_process_single_request_uses_cache():
+    client = LLMClient.basic("gpt-4o-mini")
+
+    class DictCache(dict):
+        def get(self, prompt):
+            return super().get(prompt.fingerprint)
+
+        def put(self, prompt, response):
+            self[prompt.fingerprint] = response
+
+        def __bool__(self):
+            return True
+
+    client.cache = DictCache()
+
+    context = RequestContext(
+        task_id=0,
+        model_name="gpt-4o-mini",
+        prompt=Conversation.user("hi"),
+        sampling_params=SamplingParams(),
+        attempts_left=1,
+    )
+
+    cached = APIResponse(
+        id=0,
+        status_code=200,
+        is_error=False,
+        error_message=None,
+        prompt=context.prompt,
+        content=None,
+        model_internal="gpt-4o-mini",
+        sampling_params=context.sampling_params,
+    )
+    client.cache.put(context.prompt, cached)
+
+    # Patch _execute_request to ensure it's not called
+    client._execute_request = AsyncMock()
+
+    res = asyncio.run(client.process_single_request(context))
+    assert res is cached
+    client._execute_request.assert_not_called()
+
+
+def test_process_single_request_retries_and_caches():
+    client = LLMClient.basic("gpt-4o-mini")
+
+    class DictCache(dict):
+        def get(self, prompt):
+            return super().get(prompt.fingerprint)
+
+        def put(self, prompt, response):
+            self[prompt.fingerprint] = response
+
+        def __bool__(self):
+            return True
+
+    client.cache = DictCache()
+
+    context = RequestContext(
+        task_id=0,
+        model_name="gpt-4o-mini",
+        prompt=Conversation.user("hi"),
+        sampling_params=SamplingParams(),
+        attempts_left=2,
+        all_model_names=["gpt-4o-mini", "gpt-4.1-mini"],
+        all_sampling_params=[SamplingParams(), SamplingParams()],
+    )
+
+    first = APIResponse(
+        id=0,
+        status_code=500,
+        is_error=True,
+        error_message="fail",
+        prompt=context.prompt,
+        model_internal="gpt-4o-mini",
+        sampling_params=context.sampling_params,
+        content=None,
+    )
+    from lm_deluge.prompt import Message, Text
+
+    second = APIResponse(
+        id=0,
+        status_code=200,
+        is_error=False,
+        error_message=None,
+        prompt=context.prompt,
+        model_internal="gpt-4.1-mini",
+        sampling_params=context.sampling_params,
+        content=Message("assistant", [Text("ok")]),
+    )
+    client._execute_request = AsyncMock(side_effect=[first, second])
+
+    res = asyncio.run(client.process_single_request(context))
+    assert res is second
+    assert client.cache.get(context.prompt) is second
+    assert client._execute_request.call_count == 2
+
+
+if __name__ == "__main__":
+    asyncio.run(test_process_single_request_uses_cache())
+    asyncio.run(test_process_single_request_retries_and_caches())
