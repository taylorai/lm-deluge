diff --git a/src/lm_deluge/api_requests/base.py b/src/lm_deluge/api_requests/base.py
index 2c7c01f4aad36fdbd5e6309fa52f837d23f355bb..0d7a68b3ca323e4aa881ce4d07adb823d84966ba 100644
--- a/src/lm_deluge/api_requests/base.py
+++ b/src/lm_deluge/api_requests/base.py
@@ -2,66 +2,62 @@ import asyncio
 import random
 import traceback
 from abc import ABC, abstractmethod

 import aiohttp
 from aiohttp import ClientResponse

 from lm_deluge.models import APIModel

 from ..config import SamplingParams
 from ..errors import raise_if_modal_exception
 from ..request_context import RequestContext
 from .response import APIResponse


 class APIRequestBase(ABC):
     """
     Class for handling API requests. All model/endpoint-specific logic should be
     handled by overriding __init__ and implementing the handle_response method.
     For call_api to work, the __init__ must handle setting:
         - url
         - request_header
         - request_json
     """

-    def __init__(
-        self,
-        context: RequestContext,
-    ):
-        # If context is provided, use it; otherwise construct one from individual parameters
+    def __init__(self, context: RequestContext):
+        """Base initializer storing the request context."""
         self.context = context

-        # Everything is now accessed through self.context - no copying!
-        self.system_prompt = None
-        self.result = []  # list of APIResponse objects from each attempt
+        # Result storage is optional but kept for compatibility with older code
+        self.result: list[APIResponse] = []

-        # these should be set in the __init__ of the subclass
-        self.url = None
-        self.request_header = None
-        self.request_json = None
-        self.region = None
+        # These will be populated by subclasses
+        self.url: str | None = None
+        self.request_header: dict | None = None
+        self.request_json: dict | None = None
+        self.region: str | None = None

     def increment_pbar(self):
         if self.context.status_tracker:
             self.context.status_tracker.increment_pbar()

     def call_callback(self):
         if self.context.callback is not None:
             # the APIResponse in self.result includes all the information
             self.context.callback(self.result[-1], self.context.status_tracker)

     def handle_success(self, data):
         self.call_callback()
         if self.context.status_tracker:
             self.context.status_tracker.task_succeeded(self.context.task_id)

     def handle_error(self, create_new_request=False, give_up_if_no_other_models=False):
         """
         If create_new_request is True, will create a new API request (so that it
         has a chance of being sent to a different model). If false, will retry
         the same request.
         """
         assert self.context.status_tracker
         last_result: APIResponse = self.result[-1]
         error_to_print = f"Error  task {self.context.task_id}. "
         error_to_print += (
@@ -110,107 +106,108 @@ class APIRequestBase(ABC):
                         ]
                     elif isinstance(self.context.all_sampling_params, SamplingParams):
                         new_sampling_params = self.context.all_sampling_params
                     elif self.context.all_sampling_params is None:
                         new_sampling_params = self.context.sampling_params
                     else:
                         new_sampling_params = self.context.sampling_params

                     print("Creating new request with model", new_model_name)
                     # Create new context with updated model and sampling params
                     new_context = self.context.copy(
                         model_name=new_model_name, sampling_params=new_sampling_params
                     )
                     new_model_obj = APIModel.from_registry(new_model_name)
                     new_request = new_model_obj.make_request(new_context)
                     # PROBLEM: new request is never put into results array, so we can't get the result.
                     assert self.context.status_tracker.retry_queue
                     self.context.status_tracker.retry_queue.put_nowait(self)
                     # SOLUTION: just need to make sure it's deduplicated by task_id later.
                     assert self.context.results_arr
                     self.context.results_arr.append(new_request)
         else:
             print(f"Task {self.context.task_id} out of tries.")
             self.context.status_tracker.task_failed(self.context.task_id)

-    async def call_api(self):
+    async def execute_once(self) -> APIResponse:
+        """Send the HTTP request once and return the parsed APIResponse."""
         assert self.context.status_tracker
         try:
             self.context.status_tracker.total_requests += 1
             timeout = aiohttp.ClientTimeout(total=self.context.request_timeout)
             async with aiohttp.ClientSession(timeout=timeout) as session:
                 assert self.url is not None, "URL is not set"
                 async with session.post(
                     url=self.url,
                     headers=self.request_header,
                     json=self.request_json,
                 ) as http_response:
                     response: APIResponse = await self.handle_response(http_response)

             self.result.append(response)
-            if response.is_error:
-                self.handle_error(
-                    create_new_request=response.retry_with_different_model or False,
-                    give_up_if_no_other_models=response.give_up_if_no_other_models
-                    or False,
-                )
-            else:
-                self.handle_success(response)
+            return response

         except asyncio.TimeoutError:
-            self.result.append(
-                APIResponse(
-                    id=self.context.task_id,
-                    model_internal=self.context.model_name,
-                    prompt=self.context.prompt,
-                    sampling_params=self.context.sampling_params,
-                    status_code=None,
-                    is_error=True,
-                    error_message="Request timed out (terminated by client).",
-                    content=None,
-                    usage=None,
-                )
+            response = APIResponse(
+                id=self.context.task_id,
+                model_internal=self.context.model_name,
+                prompt=self.context.prompt,
+                sampling_params=self.context.sampling_params,
+                status_code=None,
+                is_error=True,
+                error_message="Request timed out (terminated by client).",
+                content=None,
+                usage=None,
             )
-            self.handle_error(create_new_request=False)
+            self.result.append(response)
+            return response

         except Exception as e:
             raise_if_modal_exception(e)
             tb = traceback.format_exc()
             print(tb)
-            self.result.append(
-                APIResponse(
-                    id=self.context.task_id,
-                    model_internal=self.context.model_name,
-                    prompt=self.context.prompt,
-                    sampling_params=self.context.sampling_params,
-                    status_code=None,
-                    is_error=True,
-                    error_message=f"Unexpected {type(e).__name__}: {str(e) or 'No message.'}",
-                    content=None,
-                    usage=None,
-                )
+            response = APIResponse(
+                id=self.context.task_id,
+                model_internal=self.context.model_name,
+                prompt=self.context.prompt,
+                sampling_params=self.context.sampling_params,
+                status_code=None,
+                is_error=True,
+                error_message=f"Unexpected {type(e).__name__}: {str(e) or 'No message.'}",
+                content=None,
+                usage=None,
+            )
+            self.result.append(response)
+            return response
+
+    async def call_api(self):
+        response = await self.execute_once()
+        if response.is_error:
+            self.handle_error(
+                create_new_request=response.retry_with_different_model or False,
+                give_up_if_no_other_models=response.give_up_if_no_other_models or False,
             )
-            # maybe consider making True?
-            self.handle_error(create_new_request=False)
+        else:
+            self.handle_success(response)

     @abstractmethod
     async def handle_response(self, http_response: ClientResponse) -> APIResponse:
         raise NotImplementedError


 def deduplicate_responses(results: list[APIRequestBase]) -> list[APIResponse]:
     deduplicated = {}
     for request in results:
         if request.context.task_id not in deduplicated:
             deduplicated[request.context.task_id] = request.result[-1]
         else:
             current_response: APIResponse = deduplicated[request.context.task_id]
             # only replace if the current request has no completion and the new one does
             if (
                 request.result[-1].completion is not None
                 and current_response.completion is None
             ):
                 deduplicated[request.context.task_id] = request.result[-1]

     output = [deduplicated[request.context.task_id] for request in results]

     return output
diff --git a/src/lm_deluge/client.py b/src/lm_deluge/client.py
index e561914dac27f77102e25f289b5f516ab0363690..77652c66f602374c9ca0999fdaad5f08ef313c10 100644
--- a/src/lm_deluge/client.py
+++ b/src/lm_deluge/client.py
@@ -152,50 +152,150 @@ class LLMClient(BaseModel):
         else:
             config_dict["sampling_params"] = SamplingParams(
                 **config_dict["sampling_params"]
             )

         return cls(**config_dict)

     @classmethod
     def from_yaml(cls, file_path: str):
         config_dict = yaml.safe_load(open(file_path))
         return cls.from_dict(config_dict)

     @classmethod
     def basic(cls, model: str | list[str], **kwargs):
         """
         Doesn't do anything differently now, kept for backwards compat.
         """
         kwargs["model_names"] = model
         return cls(**kwargs)

     def _select_model(self):
         assert isinstance(self.model_weights, list)
         model_idx = np.random.choice(range(len(self.models)), p=self.model_weights)
         return self.models[model_idx], self.sampling_params[model_idx]

+    async def _execute_request(self, context: RequestContext) -> APIResponse:
+        """Create and send a single API request using the provided context."""
+        model_obj = APIModel.from_registry(context.model_name)
+        request = model_obj.make_request(context)
+        response = await request.execute_once()
+        return response
+
+    async def process_single_request(self, context: RequestContext) -> APIResponse:
+        """Handle caching and retries for a single request."""
+        if self.cache:
+            cached = self.cache.get(context.cache_key)
+            if cached:
+                if context.status_tracker:
+                    context.status_tracker.task_succeeded(context.task_id)
+                cached.cache_hit = True
+                return cached
+
+        response: APIResponse | None = None
+        for attempt in range(context.attempts_left):
+            response = await self._execute_request(context)
+            if not response.is_error:
+                break
+
+            # If we still have attempts left, maybe switch models
+            if attempt < context.attempts_left - 1 and context.all_model_names and len(context.all_model_names) > 1:
+                idx = np.random.randint(0, len(context.all_model_names))
+                context.model_name = context.all_model_names[idx]
+                if isinstance(context.all_sampling_params, list):
+                    context.sampling_params = context.all_sampling_params[idx]
+
+        assert response is not None
+
+        if self.cache and response.completion:
+            self.cache.put(context.cache_key, response)
+
+        if context.status_tracker:
+            if response.is_error:
+                context.status_tracker.task_failed(context.task_id)
+            else:
+                context.status_tracker.task_succeeded(context.task_id)
+
+        return response
+
+    async def process_prompts_simple_async(
+        self,
+        prompts: Sequence[str | list[dict] | Conversation],
+        *,
+        tools: list[Tool] | None = None,
+        cache: CachePattern | None = None,
+        computer_use: bool = False,
+        display_width: int = 1024,
+        display_height: int = 768,
+        use_responses_api: bool = False,
+    ) -> list[APIResponse]:
+        """Simpler processing loop where the client manages request retries."""
+
+        prompts = prompts_to_conversations(prompts)
+
+        tracker = StatusTracker(
+            max_requests_per_minute=self.max_requests_per_minute,
+            max_tokens_per_minute=self.max_tokens_per_minute,
+            max_concurrent_requests=self.max_concurrent_requests,
+            use_progress_bar=True,
+            progress_bar_total=len(prompts),
+            progress_bar_disable=False,
+        )
+
+        tracker.init_progress_bar()
+
+        results: list[APIResponse] = []
+        for task_id, prompt in enumerate(prompts):
+            model_name, sampling_params = self._select_model()
+            context = RequestContext(
+                task_id=task_id,
+                model_name=model_name,
+                prompt=prompt,
+                sampling_params=sampling_params,
+                attempts_left=self.max_attempts,
+                request_timeout=self.request_timeout,
+                status_tracker=tracker,
+                all_model_names=self.models,
+                all_sampling_params=self.sampling_params,
+                tools=tools,
+                cache=cache,
+                computer_use=computer_use,
+                display_width=display_width,
+                display_height=display_height,
+                use_responses_api=use_responses_api,
+            )
+
+            tracker.update_capacity()
+            tracker.check_capacity(context.num_tokens)
+
+            result = await self.process_single_request(context)
+            results.append(result)
+
+        tracker.log_final_status()
+
+        return results
+
     @overload
     async def process_prompts_async(
         self,
         prompts: Sequence[str | list[dict] | Conversation],
         *,
         return_completions_only: Literal[True],
         show_progress: bool = ...,
         tools: list[Tool] | None = ...,
         cache: CachePattern | None = ...,
         computer_use: bool = ...,
         display_width: int = ...,
         display_height: int = ...,
         use_responses_api: bool = ...,
     ) -> list[str | None]: ...

     @overload
     async def process_prompts_async(
         self,
         prompts: Sequence[str | list[dict] | Conversation],
         *,
         return_completions_only: Literal[False] = ...,
         show_progress: bool = ...,
         tools: list[Tool] | None = ...,
         cache: CachePattern | None = ...,
         computer_use: bool = ...,
diff --git a/tests/test_client_managed_requests.py b/tests/test_client_managed_requests.py
new file mode 100644
index 0000000000000000000000000000000000000000..4e1b8a184994963ada63e957a2f716e4e311f52d
--- /dev/null
+++ b/tests/test_client_managed_requests.py
@@ -0,0 +1,105 @@
+import asyncio
+from unittest.mock import patch
+import sys
+
+# Patch tiktoken before importing modules that require it to avoid network calls
+class _FakeTok:
+    def encoding_for_model(self, _):
+        class Dummy:
+            def encode(self, text):
+                return [0] * len(text.split())
+
+        return Dummy()
+
+
+sys.modules.setdefault("tiktoken", _FakeTok())
+
+from lm_deluge import Conversation, LLMClient
+from lm_deluge.api_requests.base import APIResponse
+from lm_deluge.prompt import Message, Text
+from lm_deluge.request_context import RequestContext
+from lm_deluge.config import SamplingParams
+from lm_deluge.tracker import StatusTracker
+
+
+class DictCache:
+    def __init__(self):
+        self.store = {}
+
+    def get(self, key):
+        return self.store.get(key)
+
+    def put(self, key, value):
+        self.store[key] = value
+
+
+async def _async_test():
+    client = LLMClient.basic("gpt-4o-mini")
+    cache = DictCache()
+    client.cache = cache
+    tracker = StatusTracker(
+        max_requests_per_minute=10,
+        max_tokens_per_minute=1000,
+        max_concurrent_requests=5,
+        use_progress_bar=False,
+    )
+
+    prompt = Conversation.user("hi")
+    context = RequestContext(
+        task_id=0,
+        model_name="gpt-4o-mini",
+        prompt=prompt,
+        sampling_params=SamplingParams(),
+        attempts_left=2,
+        status_tracker=tracker,
+        cache=cache,
+        all_model_names=["gpt-4o-mini"],
+    )
+
+    error_resp = APIResponse(
+        id=0,
+        model_internal="gpt-4o-mini",
+        prompt=prompt,
+        sampling_params=SamplingParams(),
+        status_code=500,
+        is_error=True,
+        error_message="fail",
+        content=None,
+        usage=None,
+    )
+    success_resp = APIResponse(
+        id=0,
+        model_internal="gpt-4o-mini",
+        prompt=prompt,
+        sampling_params=SamplingParams(),
+        status_code=200,
+        is_error=False,
+        error_message=None,
+        content=Message("assistant", [Text("ok")]),
+        usage=None,
+    )
+
+    async def side_effect(ctx):
+        if side_effect.calls == 0:
+            side_effect.calls += 1
+            return error_resp
+        return success_resp
+
+    side_effect.calls = 0
+
+    with patch.object(LLMClient, "_execute_request", side_effect=side_effect):
+        res = await client.process_single_request(context)
+        assert res is success_resp
+        assert tracker.num_tasks_succeeded == 1
+        # Second call should hit cache
+        res2 = await client.process_single_request(context)
+        assert res2.cache_hit
+
+
+def test_client_managed_request():
+    asyncio.run(_async_test())
+
+
+if __name__ == "__main__":
+    asyncio.run(_async_test())
+
