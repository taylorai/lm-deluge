diff --git a/src/lm_deluge/api_requests/base.py b/src/lm_deluge/api_requests/base.py
index 2c7c01f4aad36fdbd5e6309fa52f837d23f355bb..4fa31cf25d903377152533233c9d96ffc2beda7d 100644
--- a/src/lm_deluge/api_requests/base.py
+++ b/src/lm_deluge/api_requests/base.py
@@ -19,50 +19,63 @@ class APIRequestBase(ABC):
     Class for handling API requests. All model/endpoint-specific logic should be
     handled by overriding __init__ and implementing the handle_response method.
     For call_api to work, the __init__ must handle setting:
         - url
         - request_header
         - request_json
     """

     def __init__(
         self,
         context: RequestContext,
     ):
         # If context is provided, use it; otherwise construct one from individual parameters
         self.context = context

         # Everything is now accessed through self.context - no copying!
         self.system_prompt = None
         self.result = []  # list of APIResponse objects from each attempt

         # these should be set in the __init__ of the subclass
         self.url = None
         self.request_header = None
         self.request_json = None
         self.region = None

+    async def send(self) -> APIResponse:
+        """Send the HTTP request once and return the parsed APIResponse."""
+        timeout = aiohttp.ClientTimeout(total=self.context.request_timeout)
+        async with aiohttp.ClientSession(timeout=timeout) as session:
+            assert self.url is not None, "URL is not set"
+            async with session.post(
+                url=self.url,
+                headers=self.request_header,
+                json=self.request_json,
+            ) as http_response:
+                response: APIResponse = await self.handle_response(http_response)
+        return response
+
     def increment_pbar(self):
         if self.context.status_tracker:
             self.context.status_tracker.increment_pbar()

     def call_callback(self):
         if self.context.callback is not None:
             # the APIResponse in self.result includes all the information
             self.context.callback(self.result[-1], self.context.status_tracker)

     def handle_success(self, data):
         self.call_callback()
         if self.context.status_tracker:
             self.context.status_tracker.task_succeeded(self.context.task_id)

     def handle_error(self, create_new_request=False, give_up_if_no_other_models=False):
         """
         If create_new_request is True, will create a new API request (so that it
         has a chance of being sent to a different model). If false, will retry
         the same request.
         """
         assert self.context.status_tracker
         last_result: APIResponse = self.result[-1]
         error_to_print = f"Error  task {self.context.task_id}. "
         error_to_print += (
             f"Model: {last_result.model_internal} Code: {last_result.status_code}, "
diff --git a/src/lm_deluge/api_requests/bedrock.py b/src/lm_deluge/api_requests/bedrock.py
index 84320bca282f0c84fd41ee59397dcac5d26753d5..ce6dc4faea2ebe9a20ad9ea718919582f56663d9 100644
--- a/src/lm_deluge/api_requests/bedrock.py
+++ b/src/lm_deluge/api_requests/bedrock.py
@@ -104,50 +104,75 @@ class BedrockRequest(APIRequestBase):
             if self.context.tools:
                 tool_definitions.extend(
                     [tool.dump_for("anthropic") for tool in self.context.tools]
                 )

             # Add cache control to last tool if tools_only caching is specified
             if self.context.cache == "tools_only" and tool_definitions:
                 tool_definitions[-1]["cache_control"] = {"type": "ephemeral"}

             self.request_json["tools"] = tool_definitions

         # Setup AWS4Auth for signing
         self.auth = AWS4Auth(
             self.access_key,
             self.secret_key,
             self.region,
             self.service,
             session_token=self.session_token,
         )

         # Setup basic headers (AWS4Auth will add the Authorization header)
         self.request_header = {
             "Content-Type": "application/json",
         }

+    async def send(self) -> APIResponse:
+        """Send the request using AWS4Auth signing and return APIResponse."""
+        import aiohttp
+        payload = json.dumps(self.request_json, separators=(",", ":")).encode("utf-8")
+        import requests
+
+        fake_request = requests.Request(
+            method="POST",
+            url=self.url,
+            data=payload,
+            headers=self.request_header.copy(),
+        )
+        prepared_request = fake_request.prepare()
+        signed_request = self.auth(prepared_request)
+        signed_headers = dict(signed_request.headers)
+        timeout = aiohttp.ClientTimeout(total=self.context.request_timeout)
+        async with aiohttp.ClientSession(timeout=timeout) as session:
+            async with session.post(
+                url=self.url,
+                headers=signed_headers,
+                data=payload,
+            ) as http_response:
+                response: APIResponse = await self.handle_response(http_response)
+        return response
+
     async def call_api(self):
         """Override call_api to handle AWS4Auth signing."""
         try:
             import aiohttp

             assert self.context.status_tracker

             self.context.status_tracker.total_requests += 1
             timeout = aiohttp.ClientTimeout(total=self.context.request_timeout)

             # Prepare the request data
             payload = json.dumps(self.request_json, separators=(",", ":")).encode(
                 "utf-8"
             )

             # Create a fake requests.PreparedRequest object for AWS4Auth to sign
             import requests

             fake_request = requests.Request(
                 method="POST",
                 url=self.url,
                 data=payload,
                 headers=self.request_header.copy(),
             )

diff --git a/src/lm_deluge/client.py b/src/lm_deluge/client.py
index e561914dac27f77102e25f289b5f516ab0363690..ca4e94f9dea7ef5874f16692b052d3af0f1c98c2 100644
--- a/src/lm_deluge/client.py
+++ b/src/lm_deluge/client.py
@@ -338,50 +338,118 @@ class LLMClient(BaseModel):
                 if tracker.seconds_to_pause > 0:
                     await asyncio.sleep(tracker.seconds_to_pause)
                     print(f"Pausing {tracker.seconds_to_pause}s to cool down.")

             # after finishing, log final status
             tracker.log_final_status()

             # deduplicate results by id
             api_results = deduplicate_responses(requests)
             for res in api_results:
                 results[res.id] = res
                 # set to cache if result has a completion
                 if self.cache and res.completion:
                     self.cache.put(prompts[res.id], res)

         # add cache hits back in
         for id, res in zip(cache_hit_ids, cache_hit_results):
             res.cache_hit = True
             results[id] = res

         if return_completions_only:
             return [r.completion if r is not None else None for r in results]

         return results

+    async def process_prompts_async_v2(
+        self,
+        prompts: Sequence[str | list[dict] | Conversation],
+        *,
+        return_completions_only: bool = False,
+        show_progress: bool = True,
+        tools: list[Tool] | None = None,
+        cache: CachePattern | None = None,
+        computer_use: bool = False,
+        display_width: int = 1024,
+        display_height: int = 768,
+        use_responses_api: bool = False,
+    ) -> list[APIResponse | None] | list[str | None]:
+        prompts = prompts_to_conversations(prompts)
+        results: list[APIResponse | None] = [None for _ in range(len(prompts))]
+
+        tracker = StatusTracker(
+            max_requests_per_minute=self.max_requests_per_minute,
+            max_tokens_per_minute=self.max_tokens_per_minute,
+            max_concurrent_requests=self.max_concurrent_requests,
+            use_progress_bar=show_progress,
+            progress_bar_total=len(prompts),
+            progress_bar_disable=not show_progress,
+            use_rich=show_progress,
+        )
+        tracker.init_progress_bar()
+
+        async def worker(task_id: int, prompt: Conversation):
+            model, sampling_params = self._select_model()
+            context = RequestContext(
+                task_id=task_id,
+                model_name=model,
+                prompt=prompt,
+                sampling_params=sampling_params,
+                attempts_left=self.max_attempts,
+                request_timeout=self.request_timeout,
+                status_tracker=tracker,
+                all_model_names=self.models,
+                all_sampling_params=self.sampling_params,
+                tools=tools,
+                cache=cache,
+                computer_use=computer_use,
+                display_width=display_width,
+                display_height=display_height,
+                use_responses_api=use_responses_api,
+            )
+
+            while True:
+                tracker.update_capacity()
+                if tracker.check_capacity(context.num_tokens):
+                    break
+                await asyncio.sleep(0.05)
+
+            tracker.start_task(task_id)
+            resp = await self.process_single_request(context)
+            if resp.is_error:
+                tracker.task_failed(task_id)
+            else:
+                tracker.task_succeeded(task_id)
+            results[task_id] = resp
+
+        await asyncio.gather(*(worker(i, p) for i, p in enumerate(prompts)))
+        tracker.log_final_status()
+
+        if return_completions_only:
+            return [r.completion if r is not None else None for r in results]
+        return results
+
     def process_prompts_sync(
         self,
         prompts: Sequence[str | list[dict] | Conversation],
         *,
         return_completions_only: bool = False,
         show_progress=True,
         tools: list[Tool] | None = None,
         cache: CachePattern | None = None,
     ):
         return asyncio.run(
             self.process_prompts_async(
                 prompts=prompts,
                 return_completions_only=return_completions_only,
                 show_progress=show_progress,
                 tools=tools,
                 cache=cache,
             )
         )

     async def stream(self, prompt: str | Conversation, tools: list[Tool] | None = None):
         model, sampling_params = self._select_model()
         if isinstance(prompt, str):
             prompt = Conversation.user(prompt)
         async for item in stream_chat(model, prompt, sampling_params, tools, None):
             if isinstance(item, str):
