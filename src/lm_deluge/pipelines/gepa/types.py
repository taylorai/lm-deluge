"""
Core types for the GEPA module.

These types are intentionally flexible to accommodate different use cases:
- Simple Q&A evaluation
- Complex multi-step agent rollouts
- Integration with verifiers library
"""

from __future__ import annotations

from dataclasses import dataclass, field
from typing import Any, Generic, TypeVar

# Type variables for user-defined types
DataInstance = TypeVar("DataInstance")
"""User-defined type for input data to the program under optimization."""

Trajectory = TypeVar("Trajectory")
"""User-defined type capturing execution traces for reflection."""

RolloutOutput = TypeVar("RolloutOutput")
"""User-defined type for program output (can be any type)."""

# Core types
Candidate = dict[str, str]
"""A candidate program is a mapping from component names to component text."""

DataId = TypeVar("DataId")
"""Identifier type for data instances (usually int or str)."""


@dataclass
class TrajectoryRecord:
    """
    A single trajectory record with inputs, outputs, and feedback.

    This is the format expected by the reflection system. Each record
    should contain enough context to understand what happened and
    suggest improvements.

    Attributes:
        inputs: The inputs provided to the component (can be nested dict)
        outputs: The outputs generated by the component
        feedback: Feedback on the component's performance (score, errors, etc.)
        extra: Any additional metadata
    """

    inputs: dict[str, Any]
    outputs: Any
    feedback: str
    extra: dict[str, Any] = field(default_factory=dict)

    def to_dict(self) -> dict[str, Any]:
        """Convert to the format expected by reflection prompts."""
        result = {
            "Inputs": self.inputs,
            "Generated Outputs": self.outputs,
            "Feedback": self.feedback,
        }
        result.update(self.extra)
        return result


@dataclass
class EvaluationBatch(Generic[Trajectory, RolloutOutput]):
    """
    Container for evaluation results on a batch of data.

    This is the return type from Evaluator.evaluate(). It contains:
    - outputs: raw per-example outputs from the program
    - scores: per-example numeric scores (higher is better)
    - trajectories: optional execution traces for reflection

    The GEPA engine uses:
    - sum(scores) on minibatches for acceptance decisions
    - mean(scores) on validation set for tracking/pareto fronts

    Attributes:
        outputs: Raw per-example outputs (len == batch size)
        scores: Per-example float scores (len == batch size, higher is better)
        trajectories: Optional per-example traces for reflection (len == batch size if provided)
    """

    outputs: list[RolloutOutput]
    scores: list[float]
    trajectories: list[Trajectory] | None = None

    def __post_init__(self):
        assert (
            len(self.outputs) == len(self.scores)
        ), f"outputs and scores must have same length, got {len(self.outputs)} and {len(self.scores)}"
        if self.trajectories is not None:
            assert (
                len(self.trajectories) == len(self.scores)
            ), f"trajectories must match batch size, got {len(self.trajectories)} and {len(self.scores)}"

    @property
    def avg_score(self) -> float:
        """Average score across the batch."""
        if not self.scores:
            return 0.0
        return sum(self.scores) / len(self.scores)

    @property
    def sum_score(self) -> float:
        """Sum of scores across the batch."""
        return sum(self.scores)


@dataclass
class ReflectiveDataset:
    """
    Dataset for driving instruction refinement.

    Maps component names to lists of trajectory records that will
    be shown to the reflection LLM to propose improvements.
    """

    data: dict[str, list[TrajectoryRecord]]

    def __getitem__(self, key: str) -> list[TrajectoryRecord]:
        return self.data.get(key, [])

    def __contains__(self, key: str) -> bool:
        return key in self.data and len(self.data[key]) > 0

    def get(
        self, key: str, default: list[TrajectoryRecord] | None = None
    ) -> list[TrajectoryRecord] | None:
        if key in self.data and self.data[key]:
            return self.data[key]
        return default

    def keys(self) -> list[str]:
        return list(self.data.keys())

    def to_dict(self) -> dict[str, list[dict[str, Any]]]:
        """Convert to format expected by reflection prompts."""
        return {
            name: [record.to_dict() for record in records]
            for name, records in self.data.items()
        }
